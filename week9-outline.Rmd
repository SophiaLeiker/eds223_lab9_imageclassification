# Overview
Monitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance. 

Classifying remotely sensed imagery into landcover classes enables us to understand the distribution and change in landcover types over large areas. There are many approaches for performing landcover classification -- *supervised* approaches use training data labeled by the user, whereas *unsupervised* approaches use algorithms to create groups which are identified by the user afterward.\

credit: this lab is based on a materials developed by Chris Kibler.

# Task
In this lab, we are using a form of supervised classification, a *decision tree classifier*. [Decision trees](https://medium.com/@ml.at.berkeley/machine-learning-crash-course-part-5-decision-trees-and-ensemble-models-dcc5a36af8cd) classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data. In this lab we will create a land cover classification for southern Santa Barbara County based on multi-spectral imagery and data on the location of 4 land cover types:

-   green vegetation\
-   dry grass or soil\
-   urban\
-   water\

## Summary

-   load and process Landsat scene\
-   crop and mask Landsat data to study area\
-   extract spectral data at training sites\
-   train and apply decision tree classifier\
-   plot results

## Data

**Landsat 5 Thematic Mapper**\

-   [Landsat 5](https://www.usgs.gov/landsat-missions/landsat-5)
-   1 scene from September 25, 2007\
-   bands: 1, 2, 3, 4, 5, 7
-   Collection 2 surface reflectance product\

**Study area and training data**

-   polygon representing southern Santa Barbara county
-   polygons representing training sites\
    - type: character string with land cover type\

# Workflow
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Process data
#### Load packages and set working directory
We'll be working with vector and raster data, so will need both `sf` and `terra`. To train our classification algorithm and plot the results, we'll use the `rpart` and `rpart.plot` packages.
Set your working directory to the folder that holds the data for this lab.\

**Note:** my filepaths may look different than yours!
```{r include=TRUE, message=FALSE, warning=FALSE}
library(sf)
library(terra)
library(here)
library(dplyr)
library(rpart)
library(rpart.plot)
library(tmap)
library(here)

rm(list = ls())

#here::i_am("labs/data/week9.Rmd")
#setwd(here())
```

#### Load Landsat data
Let's create a raster stack based on the 6 bands we will be working with. Each file name ends with the band number (e.g. `B1.tif`). Notice that we are missing a file for band 6. Band 6 corresponds to thermal data, which we will not be working with for this lab. To create a raster stack, we will create a list of the files that we would like to work with and read them all in at once using the `rast` function. We'll then update the names of the layers to match the spectral bands and plot a true color image to see what we're working with.

```{r include=TRUE}
# list files for each band, including full file path
filelist <- list.files(here("data", "landsat-data"), full.names = TRUE)
filelist
```

#### Load study area
We want to contstrain our analysis to the southern portion of the county where we have training data, so we'll read in a file that defines the area we would like to study.

```{r include=TRUE}
#reading in rasters and putting them in a stack
landsat_20070925 <- rast(filelist)

#Updating the names of the layers
#this is renaming the layers for the landsat image so we can easily call them
names(landsat_20070925) <- c("blue", "green", "red", "NIR", "SWIR1", "SWIR2")

#Plotting red, green, blue image by indicating what layers is aligned with each bandred data is stored in layer 3, green layer is in layer 2
#then doing a linear stretch
plotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = "lin")
```

#### Crop and mask Landsat data to study area
Now, we can crop and mask the Landsat data to our study area. This reduces the amount of data we'll be working with and therefore saves computational time. We can also remove any objects we're no longer working with to save space. 
```{r include=TRUE}
#Cropping to just a southern region of Santa Barbara
SB_county_south <- st_read(here("data", "SB_county_south.shp"))

#reprojecting using a crs to match the crs of the landsat image (now it should be in WGS 84 UTM 11)
SB_county_south <- st_transform(SB_county_south, crs = crs(landsat_20070925))

#cropping the landsat image to the extent of the shape file (here there is still a rectangle around it and for each of those pixels there is still a value of NA)
#cropping the landsat scene using the SB_county_south as the cropping image, crop it down to the same extent (get rid of any rows or columns that we don't need)
landsat_cropped <- crop(landsat_20070925, SB_county_south)

#masking (mask takes the rectangle and removes all the pixels we don't want so we just mask for the exact pixels that we want within the mask)
landsat_masked <- mask(landsat_cropped, SB_county_south)

#look at the output by using the plot RGB
plotRGB(landsat_masked, r = 3, g =2, b = 1, stretch = "lin")

```

#### Convert Landsat values to reflectance
Now we need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any [scaling factors](https://www.usgs.gov/faqs/how-do-i-use-scale-factor-landsat-level-2-science-products#:~:text=Landsat%20Collection%202%20surface%20temperature,the%20scale%20factor%20is%20applied.) to convert to reflectance.\

In this case, we are working with [Landsat Collection 2](https://www.usgs.gov/landsat-missions/landsat-collection-2). The valid range of pixel values for this collection 7,273-43,636, with a multiplicative scale factor of 0.0000275 and an additive scale factor of -0.2. So we reclassify any erroneous values as `NA` and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%.

```{r include=TRUE}
#correct range of values for landsat 5 are 7273 - 43626, we want to replace any values which fall outside of this with NAs

#Defining a matrix to replace bad values to NA
#Creating a reclassification matrix so negative infinity to 7273 --> set these to NA, from anything between 43636 to infinity --> set these to NA
rcl <- matrix(c(-Inf, 7273, NA,
                43636, Inf, NA),
              ncol = 3, byrow = TRUE)

#Reclassifying our raster using the reclassification matrix that we created above
landsat <- classify(landsat_masked, rcl = rcl)

#converting to reflectance based on known scale factors
# scale factor for landsat 5
# multiply by 0.0000275
# add by -0.2
#this will convert the negative values into reflectance 
landsat <- (landsat * 0.0000275 - 0.2) * 100

#plotting to check the output
plotRGB(landsat, r = 3, g = 2, b = 1, stretch = "lin")
```


## Classify image

#### Extract reflectance values for training data
We will load the shapefile identifying different locations within our study area as containing one of our 4 land cover types. We can then extract the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.

```{r include=TRUE}
training_data <- st_read("./data/trainingdata.shp") %>% 
  st_transform(., crs = crs(landsat))

#pulling out the reflectance for the pixels within each polygon
training_data_values <- extract(landsat, training_data, df = TRUE)

#creating data frame from sf object by dropping geom
training_data_attributes <- training_data %>% 
  st_drop_geometry()

#joining spectral reflectance values onto it (wanting to add landcover type to these IDs) Adding training data attributes to the training data values, and joining by IDs (they are different in terms of upper case vs lowercase)
SB_training_data <- left_join(training_data_values, training_data_attributes, 
                              by = c("ID" = "id")) %>% 
  mutate(type = as.factor(type)) #converting land cover type to factor

```

#### Train decision tree classifier
To train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are). The `rpart` function implements the [CART algorithm](https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff). The `rpart` function needs to know the model formula and training data you would like to use. Because we are performing a classification, we set `method = "class"`. We also set `na.action = na.omit` to remove any pixels with `NA`s from the analysis.\

To understand how our decision tree will classify pixels, we can plot the results. The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.   

```{r include=TRUE}
#estblish model formula
#We want to predict type based on the independent variables on the right (red, gree, blue, etc.)
SB_formula <- type ~ red + green + blue + NIR + SWIR1 + SWIR2

```

#### Apply decision tree
Now that we have created our decision tree, we can apply it to our entire image. The `terra` package includes a `predict()` function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The `predict()` function will return a raster layer with integer values. These integer values correspond to the *factor levels* in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data. 

```{r include=TRUE}
#training a decision tree classifier
SB_decision_tree <- rpart(formula = SB_formula,
                          data = SB_training_data,
                          method = "class",
                          na.action = na.omit) #omit any situation where there is NAs

#visualizing decision tree classifier
#plotted our trained decision tree
prp(SB_decision_tree)

SB_classification <- predict(landsat, SB_decision_tree,
                             type = "class", na.rm = TRUE)

#SB_classification is now a raster
#there is only one layer and that is our landcover map (our min to max values are from 1-4, each of those integers correspond to the factors for each of the land cover types. For example green vegetation was number 1, etc. )
SB_classification

```

#### Plot results
Now we can plot the results and check out our land cover map!
```{r}
tm_shape(SB_classification) +
  tm_raster(style = "cat", #it is categorical
            labels = c("green_vegetation", #reestablishing labels to reflect landcover type
                       "soil/dead grass",
                       "urban",
                       "water"),
            title = "land cover")

```
